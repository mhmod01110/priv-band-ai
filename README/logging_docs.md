# ğŸ“ Ø¯Ù„ÙŠÙ„ Ù†Ø¸Ø§Ù… Logging - Legal Policy Analyzer

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

Ù†Ø¸Ø§Ù… Logging Ù…ØªÙ‚Ø¯Ù… ÙˆØ´Ø§Ù…Ù„ ÙŠØ³Ø¬Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ:
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Prompts Ø§Ù„Ù…Ø±Ø³Ù„Ø© Ø¥Ù„Ù‰ OpenAI
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù…Ù† OpenAI
- Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„
- Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡

---

## ğŸ“‚ Ù‡ÙŠÙƒÙ„ Ù…Ø¬Ù„Ø¯ Logs

```
logs/
â”œâ”€â”€ app.log                           # Log Ø¹Ø§Ù… Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
â”œâ”€â”€ prompts/                          # Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Prompts
â”‚   â”œâ”€â”€ 20241214_153045_stage1_match_Ù…ØªØ¬Ø±_Ø§Ù„Ø£Ø²ÙŠØ§Ø¡.txt
â”‚   â”œâ”€â”€ 20241214_153050_stage2_analyze_Ù…ØªØ¬Ø±_Ø§Ù„Ø£Ø²ÙŠØ§Ø¡.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ responses/                        # Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª
â”‚   â”œâ”€â”€ 20241214_153045_stage1_match_Ù…ØªØ¬Ø±_Ø§Ù„Ø£Ø²ÙŠØ§Ø¡.json
â”‚   â”œâ”€â”€ 20241214_153050_stage2_analyze_Ù…ØªØ¬Ø±_Ø§Ù„Ø£Ø²ÙŠØ§Ø¡.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ errors/                          # Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
â”‚   â”œâ”€â”€ errors_20241214.log
â”‚   â”œâ”€â”€ error_20241214_153100.json
â”‚   â””â”€â”€ ...
â””â”€â”€ analytics/                       # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
    â”œâ”€â”€ analytics_20241214.jsonl
    â””â”€â”€ ...
```

---

## ğŸ¨ Ø£Ù†ÙˆØ§Ø¹ Logs

### 1. Console Logs (Ù…Ù„ÙˆÙ†Ø©)
ØªØ¸Ù‡Ø± ÙÙŠ Terminal Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„:

```
ğŸš€ 2024-12-14 15:30:45 - legal_policy_analyzer - INFO - Starting new analysis
ğŸ“ 2024-12-14 15:30:46 - legal_policy_analyzer - INFO - Prompt logged: stage1_match
âœ… 2024-12-14 15:30:50 - legal_policy_analyzer - INFO - Policy matched - Confidence: 95.5%
ğŸ“Š 2024-12-14 15:31:15 - legal_policy_analyzer - INFO - Analysis completed
```

Ø§Ù„Ø£Ù„ÙˆØ§Ù†:
- ğŸ”µ **DEBUG** - Cyan
- ğŸŸ¢ **INFO** - Green
- ğŸŸ¡ **WARNING** - Yellow
- ğŸ”´ **ERROR** - Red
- ğŸŸ£ **CRITICAL** - Magenta

### 2. File Logs
Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ØªÙØ­ÙØ¸ ÙÙŠ Ù…Ù„ÙØ§Øª Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹.

---

## ğŸ“‹ Ù…Ø­ØªÙˆÙ‰ Logs

### Prompt Log Files
Ù…Ù„Ù `.txt` Ù„ÙƒÙ„ Prompt ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:

```
================================================================================
PROMPT METADATA
================================================================================
{
  "timestamp": "2024-12-14T15:30:45.123456",
  "stage": "stage1_match",
  "shop_name": "Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡ Ø§Ù„Ø¹ØµØ±ÙŠØ©",
  "policy_type": "Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ùˆ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„",
  "prompt_length": 2456,
  "metadata": {
    "policy_text_length": 850
  }
}

================================================================================
PROMPT CONTENT
================================================================================
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©...
[Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù€ Prompt]
```

### Response Log Files
Ù…Ù„Ù `.json` Ù„ÙƒÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø©:

```json
{
  "timestamp": "2024-12-14T15:30:50.123456",
  "stage": "stage2_analyze",
  "shop_name": "Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡ Ø§Ù„Ø¹ØµØ±ÙŠØ©",
  "policy_type": "Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ùˆ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„",
  "response": {
    "overall_compliance_ratio": 78.5,
    "compliance_grade": "Ø¬ÙŠØ¯",
    "critical_issues": [...],
    "strengths": [...],
    "weaknesses": [...],
    "ambiguities": [...],
    "summary": "...",
    "recommendations": [...]
  },
  "metadata": {
    "overall_compliance": 78.5,
    "critical_issues_count": 2
  }
}
```

### Analytics Log Files
Ù…Ù„Ù `.jsonl` (JSON Lines) ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:

```json
{"timestamp": "2024-12-14T15:30:45", "shop_name": "Ù…ØªØ¬Ø± 1", "compliance_ratio": 78.5, "duration_seconds": 25.3, "success": true}
{"timestamp": "2024-12-14T16:45:20", "shop_name": "Ù…ØªØ¬Ø± 2", "compliance_ratio": 85.2, "duration_seconds": 22.1, "success": true}
{"timestamp": "2024-12-14T17:20:10", "shop_name": "Ù…ØªØ¬Ø± 3", "compliance_ratio": 0, "duration_seconds": 5.2, "success": false}
```

### Error Log Files
Ù…Ù„ÙØ§Øª JSON Ù„Ù„Ø£Ø®Ø·Ø§Ø¡:

```json
{
  "timestamp": "2024-12-14T15:30:55.123456",
  "error_type": "JSONDecodeError",
  "error_message": "Expecting value: line 1 column 1 (char 0)",
  "shop_name": "Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡",
  "traceback": "Traceback (most recent call last):\n  File..."
}
```

---

## ğŸ” Ø§Ø³ØªØ®Ø¯Ø§Ù… Logger ÙÙŠ Ø§Ù„ÙƒÙˆØ¯

### Ø§Ø³ØªÙŠØ±Ø§Ø¯ Logger

```python
from app.logger import app_logger
```

### Logging Ø£Ø³Ø§Ø³ÙŠ

```python
# Ø±Ø³Ø§Ø¦Ù„ Ø¨Ø³ÙŠØ·Ø©
app_logger.debug("Ø±Ø³Ø§Ù„Ø© ØªØµØ­ÙŠØ­")
app_logger.info("Ø±Ø³Ø§Ù„Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª")
app_logger.warning("ØªØ­Ø°ÙŠØ±")
app_logger.error("Ø®Ø·Ø£")
app_logger.critical("Ø®Ø·Ø£ Ø­Ø±Ø¬")
```

### Logging Prompts

```python
app_logger.log_prompt(
    stage="stage1_match",
    shop_name="Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡",
    policy_type="Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ùˆ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„",
    prompt=prompt_text,
    metadata={
        "policy_text_length": len(policy_text),
        "custom_field": "value"
    }
)
```

### Logging Responses

```python
app_logger.log_response(
    stage="stage2_analyze",
    shop_name="Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡",
    policy_type="Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ùˆ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„",
    response=response_dict,
    metadata={
        "overall_compliance": 78.5,
        "critical_issues_count": 2
    }
)
```

### Logging Analysis Summary

```python
app_logger.log_analysis_summary(
    shop_name="Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡",
    policy_type="Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ùˆ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„",
    compliance_ratio=78.5,
    duration=25.3,
    success=True
)
```

### Logging Errors

```python
import traceback

try:
    # some code
    pass
except Exception as e:
    app_logger.log_error(
        error_type=type(e).__name__,
        error_message=str(e),
        shop_name="Ù…ØªØ¬Ø± Ø§Ù„Ø£Ø²ÙŠØ§Ø¡",
        traceback_info=traceback.format_exc()
    )
```

---

## ğŸ“Š ØªØ­Ù„ÙŠÙ„ Logs

### Ù‚Ø±Ø§Ø¡Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙŠÙˆÙ…ÙŠØ©

```python
import json

# Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù analytics
with open('logs/analytics/analytics_20241214.jsonl', 'r', encoding='utf-8') as f:
    analyses = [json.loads(line) for line in f]

# Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„
avg_compliance = sum(a['compliance_ratio'] for a in analyses if a['success']) / len(analyses)
print(f"Average Compliance: {avg_compliance}%")

# Ø¹Ø¯Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
successful = sum(1 for a in analyses if a['success'])
print(f"Successful Analyses: {successful}/{len(analyses)}")

# Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©
avg_duration = sum(a['duration_seconds'] for a in analyses) / len(analyses)
print(f"Average Duration: {avg_duration:.2f} seconds")
```

### Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Prompts

```python
from pathlib import Path
import re

# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ prompts Ù„Ù…ØªØ¬Ø± Ù…Ø¹ÙŠÙ†
shop_name = "Ù…ØªØ¬Ø±_Ø§Ù„Ø£Ø²ÙŠØ§Ø¡"
prompts_dir = Path("logs/prompts")

shop_prompts = list(prompts_dir.glob(f"*{shop_name}*.txt"))
print(f"Found {len(shop_prompts)} prompts for {shop_name}")

for prompt_file in shop_prompts:
    print(f"- {prompt_file.name}")
```

### ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

```python
from pathlib import Path
import json
from collections import Counter

errors_dir = Path("logs/errors")
error_files = errors_dir.glob("error_*.json")

error_types = []
for error_file in error_files:
    with open(error_file, 'r', encoding='utf-8') as f:
        error_data = json.load(f)
        error_types.append(error_data['error_type'])

# Ø¹Ø±Ø¶ Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø´ÙŠÙˆØ¹Ø§Ù‹
error_counts = Counter(error_types)
print("Most common errors:")
for error_type, count in error_counts.most_common(5):
    print(f"  {error_type}: {count} times")
```

---

## âš™ï¸ ØªØ®ØµÙŠØµ Logger

### ØªØºÙŠÙŠØ± Ù…Ø³ØªÙˆÙ‰ Logging

```python
# ÙÙŠ app/logger.py
self.logger.setLevel(logging.DEBUG)  # Ø¹Ø±Ø¶ ÙƒÙ„ Ø´ÙŠØ¡
self.logger.setLevel(logging.INFO)   # Ø¹Ø±Ø¶ INFO ÙˆÙ…Ø§ ÙÙˆÙ‚
self.logger.setLevel(logging.WARNING) # Ø¹Ø±Ø¶ ØªØ­Ø°ÙŠØ±Ø§Øª ÙˆØ£Ø®Ø·Ø§Ø¡ ÙÙ‚Ø·
```

### Ø¥Ø¶Ø§ÙØ© Handler Ø¬Ø¯ÙŠØ¯

```python
# ÙÙŠ app/logger.py ÙÙŠ _setup_handlers()

# Ù…Ø«Ø§Ù„: Handler Ù„Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¹Ø¨Ø± Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø­Ø±Ø¬Ø©
import logging.handlers

smtp_handler = logging.handlers.SMTPHandler(
    mailhost=('smtp.example.com', 587),
    fromaddr='app@example.com',
    toaddrs=['admin@example.com'],
    subject='Legal Analyzer Critical Error'
)
smtp_handler.setLevel(logging.CRITICAL)
self.logger.addHandler(smtp_handler)
```

---

## ğŸ› ï¸ ØµÙŠØ§Ù†Ø© Logs

### ØªÙ†Ø¸ÙŠÙ Logs Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©

```python
# cleanup_logs.py
from pathlib import Path
from datetime import datetime, timedelta

def cleanup_old_logs(days_to_keep=30):
    """Ø­Ø°Ù logs Ø£Ù‚Ø¯Ù… Ù…Ù† Ø¹Ø¯Ø¯ Ù…Ø¹ÙŠÙ† Ù…Ù† Ø§Ù„Ø£ÙŠØ§Ù…"""
    
    cutoff_date = datetime.now() - timedelta(days=days_to_keep)
    logs_dir = Path("logs")
    
    for log_type in ["prompts", "responses", "errors", "analytics"]:
        type_dir = logs_dir / log_type
        if not type_dir.exists():
            continue
        
        for log_file in type_dir.iterdir():
            if log_file.is_file():
                file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
                if file_time < cutoff_date:
                    log_file.unlink()
                    print(f"Deleted: {log_file}")

if __name__ == "__main__":
    cleanup_old_logs(days_to_keep=30)
```

### Ø£Ø±Ø´ÙØ© Logs

```python
# archive_logs.py
import shutil
from pathlib import Path
from datetime import datetime

def archive_logs():
    """Ø£Ø±Ø´ÙØ© logs Ø§Ù„Ø´Ù‡Ø± Ø§Ù„Ù…Ø§Ø¶ÙŠ"""
    
    last_month = datetime.now().replace(day=1) - timedelta(days=1)
    archive_name = f"logs_archive_{last_month.strftime('%Y%m')}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø±Ø´ÙŠÙ Ù…Ø¶ØºÙˆØ·
    shutil.make_archive(
        archive_name,
        'zip',
        'logs'
    )
    
    print(f"Archive created: {archive_name}.zip")

if __name__ == "__main__":
    archive_logs()
```

---

## ğŸ“ˆ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙŠÙˆÙ…ÙŠ

```python
# daily_report.py
import json
from pathlib import Path
from datetime import datetime
from collections import defaultdict

def generate_daily_report(date_str=None):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ÙŠÙˆÙ…ÙŠ"""
    
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    analytics_file = Path(f"logs/analytics/analytics_{date_str}.jsonl")
    
    if not analytics_file.exists():
        print(f"No analytics file found for {date_str}")
        return
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    analyses = []
    with open(analytics_file, 'r', encoding='utf-8') as f:
        analyses = [json.loads(line) for line in f]
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    total = len(analyses)
    successful = sum(1 for a in analyses if a['success'])
    failed = total - successful
    
    if successful > 0:
        avg_compliance = sum(a['compliance_ratio'] for a in analyses if a['success']) / successful
        avg_duration = sum(a['duration_seconds'] for a in analyses if a['success']) / successful
    else:
        avg_compliance = 0
        avg_duration = 0
    
    # ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø³ÙŠØ§Ø³Ø©
    by_policy = defaultdict(int)
    for a in analyses:
        by_policy[a['policy_type']] += 1
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    print("=" * 80)
    print(f"ğŸ“Š Daily Report - {date_str}")
    print("=" * 80)
    print(f"Total Analyses: {total}")
    print(f"Successful: {successful} ({successful/total*100:.1f}%)")
    print(f"Failed: {failed} ({failed/total*100:.1f}%)")
    print(f"\nAverage Compliance: {avg_compliance:.1f}%")
    print(f"Average Duration: {avg_duration:.2f} seconds")
    print(f"\nAnalyses by Policy Type:")
    for policy_type, count in by_policy.items():
        print(f"  - {policy_type}: {count}")
    print("=" * 80)

if __name__ == "__main__":
    generate_daily_report()
```

---

## âœ… Best Practices

1. **Ù„Ø§ ØªØ¹Ø·Ù„ Logging ÙÙŠ Production** - Logs Ù…Ù‡Ù…Ø© Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
2. **Ø±Ø§Ù‚Ø¨ Ø­Ø¬Ù… Logs** - Ù†Ø¸Ù Ø£Ùˆ Ø£Ø±Ø´Ù Logs Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø§Ù†ØªØ¸Ø§Ù…
3. **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©**:
   - DEBUG: ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ·ÙˆÙŠØ± ÙÙ‚Ø·
   - INFO: Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ø§Ø¯ÙŠØ©
   - WARNING: Ø£Ù…ÙˆØ± ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø© Ù„ÙƒÙ† ØºÙŠØ± Ø­Ø±Ø¬Ø©
   - ERROR: Ø£Ø®Ø·Ø§Ø¡ ØªØ­ØªØ§Ø¬ Ø§Ù†ØªØ¨Ø§Ù‡
   - CRITICAL: Ø£Ø®Ø·Ø§Ø¡ Ø­Ø±Ø¬Ø© ØªØ­ØªØ§Ø¬ ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ
4. **Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø®ØµÙˆØµÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª** - Ù„Ø§ ØªØ³Ø¬Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø³Ø© Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡
5. **Ø§Ø³ØªØ®Ø¯Ù… Structured Logging** - JSON Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©

---

## ğŸ”’ Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø®ØµÙˆØµÙŠØ©

âš ï¸ **ØªØ­Ø°ÙŠØ± Ù…Ù‡Ù…:**

- Ù„Ø§ ØªØ³Ø¬Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø®ØµÙŠØ© Ø­Ø³Ø§Ø³Ø© (Ø£Ø±Ù‚Ø§Ù… Ø¨Ø·Ø§Ù‚Ø§ØªØŒ ÙƒÙ„Ù…Ø§Øª Ù…Ø±ÙˆØ±)
- ØªØ£ÙƒØ¯ Ù…Ù† Ø­Ù…Ø§ÙŠØ© Ù…Ø¬Ù„Ø¯ `logs/` Ù…Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…
- Ø§Ø³ØªØ®Ø¯Ù… `.gitignore` Ù„Ø¹Ø¯Ù… Ø±ÙØ¹ Logs Ø¹Ù„Ù‰ Git
- Ø§Ø¹ØªØ¨Ø± ØªØ´ÙÙŠØ± Logs Ø§Ù„Ø­Ø³Ø§Ø³Ø©

```gitignore
# .gitignore
logs/
*.log
```

---

**ØªÙ…! Ù†Ø¸Ø§Ù… Logging Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ‰**
